---
title: Latest 20 Papers - November 14, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## AND:multimodal LLM
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Implicit Multimodal Alignment: On the Generalization of Frozen LLMs to Multimodal Inputs](https://arxiv.org/pdf/2405.16700v2)** | 2024-10-08 | <details><summary>NeurI...</summary><p>NeurIPS 2024. Code: https://github.com/mshukor/ima-lmms. Project page: https://ima-lmms.github.io/</p></details> |
| **[Towards Vision Enhancing LLMs: Empowering Multimodal Knowledge Storage and Sharing in LLMs](https://arxiv.org/pdf/2311.15759v1)** | 2023-11-28 | 12 pages, 4 figures |
| **[LLMs Meet Multimodal Generation and Editing: A Survey](https://arxiv.org/pdf/2405.19334v2)** | 2024-06-11 | <details><summary>52 Pa...</summary><p>52 Pages with 16 Figures, 12 Tables, and 545 References. GitHub Repository at: https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation</p></details> |
| **[ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine](https://arxiv.org/pdf/2508.14706v1)** | 2025-08-21 |  |
| **[Universal Adversarial Attack on Aligned Multimodal LLMs](https://arxiv.org/pdf/2502.07987v3)** | 2025-06-06 | <details><summary>Added...</summary><p>Added benchmarks, baselines, author, appendix</p></details> |
| **[Understanding the Role of LLMs in Multimodal Evaluation Benchmarks](https://arxiv.org/pdf/2410.12329v1)** | 2024-10-17 |  |
| **[EasyGen: Easing Multimodal Generation with BiDiffuser and LLMs](https://arxiv.org/pdf/2310.08949v3)** | 2024-05-20 | <details><summary>Accep...</summary><p>Accepted by ACL 2024, main conference</p></details> |
| **[Exploring Hierarchical Molecular Graph Representation in Multimodal LLMs](https://arxiv.org/pdf/2411.04708v2)** | 2025-02-14 | <details><summary>9 pag...</summary><p>9 pages, 4 tables, 1 figure, paper under review</p></details> |
| **[From LLMs to MLLMs: Exploring the Landscape of Multimodal Jailbreaking](https://arxiv.org/pdf/2406.14859v1)** | 2024-06-24 |  |
| **[Position: Empowering Time Series Reasoning with Multimodal LLMs](https://arxiv.org/pdf/2502.01477v1)** | 2025-02-04 |  |
| **[Beyond Words: Multimodal LLM Knows When to Speak](https://arxiv.org/pdf/2505.14654v1)** | 2025-05-21 | <details><summary>Proje...</summary><p>Project page: https://github.com/lzk901372/MM-When2Speak</p></details> |
| **[Chitranuvad: Adapting Multi-Lingual LLMs for Multimodal Translation](https://arxiv.org/pdf/2502.20420v1)** | 2025-03-03 |  |
| **[Mol-LLM: Multimodal Generalist Molecular LLM with Improved Graph Utilization](https://arxiv.org/pdf/2502.02810v2)** | 2025-05-27 | 9 pages, 5 figures |
| **[LLM and GNN are Complementary: Distilling LLM for Multimodal Graph Learning](https://arxiv.org/pdf/2406.01032v1)** | 2024-06-04 |  |
| **[Discrete Tokenization for Multimodal LLMs: A Comprehensive Survey](https://arxiv.org/pdf/2507.22920v1)** | 2025-08-01 |  |
| **[Format Matters: The Robustness of Multimodal LLMs in Reviewing Evidence from Tables and Charts](https://arxiv.org/pdf/2511.10075v1)** | 2025-11-14 | <details><summary>Accep...</summary><p>Accepted at AAAI 2026</p></details> |
| **[Skipping Computations in Multimodal LLMs](https://arxiv.org/pdf/2410.09454v1)** | 2024-10-15 | <details><summary>Accep...</summary><p>Accepted at NeurIPS 2024 Workshop RBFM. Code: https://github.com/mshukor/ima-lmms</p></details> |
| **[CuMo: Scaling Multimodal LLM with Co-Upcycled Mixture-of-Experts](https://arxiv.org/pdf/2405.05949v1)** | 2024-05-10 |  |
| **[NVLM: Open Frontier-Class Multimodal LLMs](https://arxiv.org/pdf/2409.11402v2)** | 2024-10-24 | <details><summary>Fixed...</summary><p>Fixed the typos. For more information, please visit our project page at: https://research.nvidia.com/labs/adlr/NVLM-1</p></details> |
| **[Efficient Indirect LLM Jailbreak via Multimodal-LLM Jailbreak](https://arxiv.org/pdf/2405.20015v2)** | 2025-05-20 |  |

## AND:multimodal survey
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Multimodal Learning with Transformers: A Survey](https://arxiv.org/pdf/2206.06488v2)** | 2023-05-11 | <details><summary>This ...</summary><p>This paper is accepted by IEEE TPAMI</p></details> |
| **[A Survey of Multimodal Composite Editing and Retrieval](https://arxiv.org/pdf/2409.05405v2)** | 2024-09-12 | <details><summary>20 pa...</summary><p>20 pages, 3 figures, and 11 tables</p></details> |
| **[Multimodality in VR: A survey](https://arxiv.org/pdf/2101.07906v3)** | 2024-05-24 | <details><summary>35 pa...</summary><p>35 pages (24 pages not including references), 10 figures, 4 tables</p></details> |
| **[Adversarial Attacks in Multimodal Systems: A Practitioner's Survey](https://arxiv.org/pdf/2505.03084v1)** | 2025-09-03 | <details><summary>Accep...</summary><p>Accepted in IEEE COMPSAC 2025</p></details> |
| **[Multimodal Alignment and Fusion: A Survey](https://arxiv.org/pdf/2411.17040v2)** | 2025-10-14 | <details><summary>Accep...</summary><p>Accepted to IJCV 2025</p></details> |
| **[Learning on Multimodal Graphs: A Survey](https://arxiv.org/pdf/2402.05322v1)** | 2024-02-09 | 9 pages, 1 figure |
| **[A Survey on Image-text Multimodal Models](https://arxiv.org/pdf/2309.15857v3)** | 2024-06-21 |  |
| **[Deep Multimodal Learning with Missing Modality: A Survey](https://arxiv.org/pdf/2409.07825v3)** | 2024-10-22 | <details><summary>Submi...</summary><p>Submitted to ACM Computing Surveys</p></details> |
| **[A survey of multimodal deep generative models](https://arxiv.org/pdf/2207.02127v1)** | 2022-07-06 | <details><summary>Publi...</summary><p>Published in Advanced Robotics</p></details> |
| **[Multimodal Automated Fact-Checking: A Survey](https://arxiv.org/pdf/2305.13507v3)** | 2023-10-27 | <details><summary>The 2...</summary><p>The 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP): Findings</p></details> |
| **[Multimodality in Meta-Learning: A Comprehensive Survey](https://arxiv.org/pdf/2109.13576v2)** | 2022-05-10 | <details><summary>Accep...</summary><p>Accepted by Knowledge-Based Systems; 21 pages</p></details> |
| **[LLMs Meet Multimodal Generation and Editing: A Survey](https://arxiv.org/pdf/2405.19334v2)** | 2024-06-11 | <details><summary>52 Pa...</summary><p>52 Pages with 16 Figures, 12 Tables, and 545 References. GitHub Repository at: https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation</p></details> |
| **[Multimodal Referring Segmentation: A Survey](https://arxiv.org/pdf/2508.00265v2)** | 2025-08-06 | <details><summary>Proje...</summary><p>Project Page: https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation</p></details> |
| **[A Survey on Audio Synthesis and Audio-Visual Multimodal Processing](https://arxiv.org/pdf/2108.00443v1)** | 2021-08-03 |  |
| **[Multimodal Machine Learning: A Survey and Taxonomy](https://arxiv.org/pdf/1705.09406v2)** | 2017-08-02 |  |
| **[A Survey of Multimodal Sarcasm Detection](https://arxiv.org/pdf/2410.18882v1)** | 2024-10-25 | <details><summary>Publi...</summary><p>Published in the Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence Survey Track. Pages 8020-8028</p></details> |
| **[A Survey of Multimodal Retrieval-Augmented Generation](https://arxiv.org/pdf/2504.08748v1)** | 2025-04-15 |  |
| **[Recent Trends of Multimodal Affective Computing: A Survey from NLP Perspective](https://arxiv.org/pdf/2409.07388v2)** | 2024-10-31 |  |
| **[Multimodal Pretraining, Adaptation, and Generation for Recommendation: A Survey](https://arxiv.org/pdf/2404.00621v2)** | 2024-07-04 | <details><summary>Accep...</summary><p>Accepted by KDD 2024. See our tutorial materials at https://mmrec.github.io</p></details> |
| **[Brain-Conditional Multimodal Synthesis: A Survey and Taxonomy](https://arxiv.org/pdf/2401.00430v2)** | 2024-01-04 |  |

## AND:MLLM post training
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Visual Jigsaw Post-Training Improves MLLMs](https://arxiv.org/pdf/2509.25190v1)** | 2025-09-30 |  |
| **[FreeRet: MLLMs as Training-Free Retrievers](https://arxiv.org/pdf/2509.24621v1)** | 2025-09-30 |  |
| **[On Domain-Adaptive Post-Training for Multimodal Large Language Models](https://arxiv.org/pdf/2411.19930v4)** | 2025-08-28 | <details><summary>EMNLP...</summary><p>EMNLP 2025 Findings, Project Page: https://huggingface.co/AdaptLLM/Adapt-MLLM-to-Domains</p></details> |
| **[RePIC: Reinforced Post-Training for Personalizing Multi-Modal Language Models](https://arxiv.org/pdf/2506.18369v4)** | 2025-10-13 | <details><summary>Accep...</summary><p>Accepted to NeurIPS 2025</p></details> |
| **[RL makes MLLMs see better than SFT](https://arxiv.org/pdf/2510.16333v1)** | 2025-10-21 |  |
| **[Orchestrate Multimodal Data with Batch Post-Balancing to Accelerate Multimodal Large Language Model Training](https://arxiv.org/pdf/2503.23830v2)** | 2025-04-10 |  |
| **[Turning Internal Gap into Self-Improvement: Promoting the Generation-Understanding Unification in MLLMs](https://arxiv.org/pdf/2507.16663v2)** | 2025-09-26 | <details><summary>31 pa...</summary><p>31 pages, 16 figures, 12 tables</p></details> |
| **[First SFT, Second RL, Third UPT: Continual Improving Multi-Modal LLM Reasoning via Unsupervised Post-Training](https://arxiv.org/pdf/2505.22453v2)** | 2025-10-28 | <details><summary>Accep...</summary><p>Accepted by NeurIPS 2025</p></details> |
| **[Will Pre-Training Ever End? A First Step Toward Next-Generation Foundation MLLMs via Self-Improving Systematic Cognition](https://arxiv.org/pdf/2503.12303v6)** | 2025-06-17 | 43 pages. Preprint |
| **[Revisiting the Data Sampling in Multimodal Post-training from a Difficulty-Distinguish View](https://arxiv.org/pdf/2511.06722v1)** | 2025-11-11 | <details><summary>Accpe...</summary><p>Accpeted by AAAI 2026</p></details> |
| **[SafeEditor: Unified MLLM for Efficient Post-hoc T2I Safety Editing](https://arxiv.org/pdf/2510.24820v1)** | 2025-10-30 |  |
| **[An empirical study of LLaMA3 quantization: from LLMs to MLLMs](https://arxiv.org/pdf/2404.14047v3)** | 2025-01-14 |  |
| **[FairReason: Balancing Reasoning and Social Bias in MLLMs](https://arxiv.org/pdf/2507.23067v2)** | 2025-09-09 | <details><summary>Accep...</summary><p>Accepted to the Trustworthy FMs workshop in ICCV 2025</p></details> |
| **[VideoCap-R1: Enhancing MLLMs for Video Captioning via Structured Thinking](https://arxiv.org/pdf/2506.01725v1)** | 2025-06-03 |  |
| **[Threading Keyframe with Narratives: MLLMs as Strong Long Video Comprehenders](https://arxiv.org/pdf/2505.24158v1)** | 2025-06-02 |  |
| **[Urban-R1: Reinforced MLLMs Mitigate Geospatial Biases for Urban General Intelligence](https://arxiv.org/pdf/2510.16555v1)** | 2025-10-21 |  |
| **[MonetGPT: Solving Puzzles Enhances MLLMs' Image Retouching Skills](https://arxiv.org/pdf/2505.06176v1)** | 2025-05-12 | <details><summary>Accep...</summary><p>Accepted at SIGGRAPH 2025 [ACM Transactions on Graphics]; Project website: https://monetgpt.github.io</p></details> |
| **[InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models](https://arxiv.org/pdf/2504.10479v3)** | 2025-04-22 | Technical Report |
| **[Improving the Reasoning of Multi-Image Grounding in MLLMs via Reinforcement Learning](https://arxiv.org/pdf/2507.00748v2)** | 2025-07-24 | 10 pages |
| **[REALM: An MLLM-Agent Framework for Open World 3D Reasoning Segmentation and Editing on Gaussian Splatting](https://arxiv.org/pdf/2510.16410v1)** | 2025-10-21 |  |

## MLLM
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[MLLM-DataEngine: An Iterative Refinement Approach for MLLM](https://arxiv.org/pdf/2308.13566v2)** | 2023-09-12 | <details><summary>Code ...</summary><p>Code and models are available at https://github.com/opendatalab/MLLM-DataEngine</p></details> |
| **[From Specific-MLLMs to Omni-MLLMs: A Survey on MLLMs Aligned with Multi-modalities](https://arxiv.org/pdf/2412.11694v3)** | 2025-03-05 | 35 pages |
| **[MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance](https://arxiv.org/pdf/2401.02906v3)** | 2024-06-18 |  |
| **[Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence](https://arxiv.org/pdf/2505.23747v1)** | 2025-05-30 | 21 pages |
| **[AIC MLLM: Autonomous Interactive Correction MLLM for Robust Robotic Manipulation](https://arxiv.org/pdf/2406.11548v6)** | 2024-11-19 |  |
| **[Dense Connector for MLLMs](https://arxiv.org/pdf/2405.13800v2)** | 2024-11-18 | <details><summary>27 pa...</summary><p>27 pages, NeurIPS 2024</p></details> |
| **[Benchmarking Large and Small MLLMs](https://arxiv.org/pdf/2501.04150v1)** | 2025-01-09 |  |
| **[When MLLMs Meet Compression Distortion: A Coding Paradigm Tailored to MLLMs](https://arxiv.org/pdf/2509.24258v1)** | 2025-09-30 |  |
| **[The Wolf Within: Covert Injection of Malice into MLLM Societies via an MLLM Operative](https://arxiv.org/pdf/2402.14859v2)** | 2024-06-04 | <details><summary>Accep...</summary><p>Accepted to workshop on ReGenAI@CVPR 2024</p></details> |
| **[Spatial Preference Rewarding for MLLMs Spatial Understanding](https://arxiv.org/pdf/2510.14374v1)** | 2025-10-17 | ICCV 2025 |
| **[MLLMs are Deeply Affected by Modality Bias](https://arxiv.org/pdf/2505.18657v1)** | 2025-05-27 |  |
| **[Aesthetic Image Captioning with Saliency Enhanced MLLMs](https://arxiv.org/pdf/2509.04378v3)** | 2025-09-10 |  |
| **[Information Density Principle for MLLM Benchmarks](https://arxiv.org/pdf/2503.10079v1)** | 2025-03-14 |  |
| **[Redundancy Principles for MLLMs Benchmarks](https://arxiv.org/pdf/2501.13953v2)** | 2025-05-29 |  |
| **[Improve MLLM Benchmark Efficiency through Interview](https://arxiv.org/pdf/2506.00883v2)** | 2025-10-07 |  |
| **[Do MLLMs Really Understand the Charts?](https://arxiv.org/pdf/2509.04457v1)** | 2025-09-08 | 19 pages,15 figures |
| **[Face-MLLM: A Large Face Perception Model](https://arxiv.org/pdf/2410.20717v1)** | 2024-10-29 |  |
| **[RL makes MLLMs see better than SFT](https://arxiv.org/pdf/2510.16333v1)** | 2025-10-21 |  |
| **[MileBench: Benchmarking MLLMs in Long Context](https://arxiv.org/pdf/2404.18532v2)** | 2024-05-16 | <details><summary>31 pa...</summary><p>31 pages, 13 figures, 14 tables; We add results of GPT-4o in this version</p></details> |
| **[Survey on AI-Generated Media Detection: From Non-MLLM to MLLM](https://arxiv.org/pdf/2502.05240v2)** | 2025-02-13 |  |

